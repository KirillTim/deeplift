{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General import statements. REMEMBER, DEEPLIFT_DIR needs to point to the deeplift directory WITHIN the deeplift repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 750M (CNMeM is disabled, cuDNN not available)\n",
      "/Users/avantishrikumar/anaconda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division;\n",
    "from __future__ import print_function;\n",
    "from __future__ import absolute_import;\n",
    "import sys, os;\n",
    "from collections import OrderedDict, namedtuple;\n",
    "import numpy as np;\n",
    "\n",
    "#Make sure the directory is set to import the lab's version of keras\n",
    "scriptsDir = os.environ.get(\"KERAS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable KERAS_DIR\");\n",
    "sys.path.insert(0,scriptsDir)\n",
    "\n",
    "scriptsDir = os.environ.get(\"ENHANCER_SCRIPTS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable ENHANCER_SCRIPTS_DIR to point to the enhancer_prediction_code repo\");\n",
    "sys.path.insert(0,scriptsDir+\"/featureSelector/deepLIFFT\");\n",
    "from deepLIFTutils import makePngOfSequenceDeepLIFTScores\n",
    "\n",
    "import deeplift\n",
    "import deeplift.conversion.keras_conversion as kc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the keras model, make sure you normalise the weights\n",
    "#of the first convolutional layer to be mean-centered at each position.\n",
    "model_weights = \"modelsDir_100Kruns/record_0_model_jFhL6_modelWeights.h5\"\n",
    "model_yaml = \"modelsDir_100Kruns/record_0_model_jFhL6_modelYaml.yaml\"\n",
    "reload(kc)\n",
    "keras_model = kc.load_keras_model(model_weights, model_yaml, normalise_conv_for_one_hot_encoded_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'rows skipped from', 'features_100K.gz')\n",
      "Returning desired dict\n",
      "Making numpy arrays out of the loaded files\n",
      "('train', 'shapeX', (80000, 50))\n",
      "('train', 'shapeY', (80000, 45))\n",
      "('valid', 'shapeX', (10000, 50))\n",
      "('valid', 'shapeY', (10000, 45))\n",
      "('test', 'shapeX', (10000, 50))\n",
      "('test', 'shapeY', (10000, 45))\n"
     ]
    }
   ],
   "source": [
    "scriptsDir = os.environ.get(\"UTIL_SCRIPTS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable UTIL_SCRIPTS_DIR to point to the deeplift code\");\n",
    "sys.path.insert(0,scriptsDir);\n",
    "from importDataPackage import importData\n",
    "reload(importData)\n",
    "trainData, validData, testData = importData.loadTrainTestValidFromYaml(\"yaml_100K/features.yaml\",\n",
    "                                                                       \"yaml_100K/labels.yaml\",\n",
    "                                                                       \"yaml_100K/splits.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = trainData.concat(validData, testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the keras sequential model into a deeplift sequential model, and compile the functions to compute the contributions and multipliers - the multipliers are analogous to the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'deeplift.conversion.keras_conversion' from '/Users/avantishrikumar/Research/deeplift/deeplift/conversion/keras_conversion.pyc'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deeplift.blobs import MxtsMode\n",
    "reload(kc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplift_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.DeepLIFT)\n",
    "deeplift_contribs_func = deeplift_model.get_target_contribs_func(find_scores_layer_idx=0)\n",
    "deeplift_multipliers_func = deeplift_model.get_target_multipliers_func(find_scores_layer_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for other saliency map functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradients_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.Gradient)\n",
    "grad_times_inp_func = gradients_model.get_target_contribs_func(find_scores_layer_idx=0)\n",
    "grad_func = gradients_model.get_target_multipliers_func(find_scores_layer_idx=0)\n",
    "guided_backprop_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.GuidedBackprop)\n",
    "guided_backprop_times_inp_func = guided_backprop_model.get_target_contribs_func(find_scores_layer_idx=0)\n",
    "guided_backprop_func = guided_backprop_model.get_target_multipliers_func(find_scores_layer_idx=0)\n",
    "guided_backprop_deeplift_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.GuidedBackpropDeepLIFT)\n",
    "guided_backprop_deeplift_times_inp_func = guided_backprop_deeplift_model.get_target_contribs_func(find_scores_layer_idx=0)\n",
    "guided_backprop_deeplift_func = guided_backprop_deeplift_model.get_target_multipliers_func(find_scores_layer_idx=0)\n",
    "deconv_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.DeconvNet)\n",
    "deconv_func = deconv_model.get_target_multipliers_func(find_scores_layer_idx=0)\n",
    "deconv_times_inp_func = deconv_model.get_target_contribs_func(find_scores_layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "predictions_func = theano.function([deeplift_model.get_layers()[0].get_activation_vars()],\n",
    "                                   deeplift_model.get_layers()[-1].get_activation_vars(),\n",
    "                                   allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00719933,  0.00894803,  0.01022888,  0.00756006,  0.00769961,\n",
       "         0.00704683,  0.00673796,  0.00914059,  0.00965482,  0.00790531,\n",
       "         0.00867973,  0.00711914,  0.00629144,  0.00613344,  0.00732677,\n",
       "         0.00700838,  0.0096201 ,  0.01024602,  0.00887473,  0.00915498,\n",
       "         0.00766505,  0.00867055,  0.00903935,  0.00873579,  0.00890495,\n",
       "         0.00911928,  0.0095769 ,  0.00908662,  0.00882156,  0.01010747,\n",
       "         0.00655401,  0.00731878,  0.00805646,  0.00802046,  0.00947253,\n",
       "         0.00848896,  0.00730444,  0.00761912,  0.01101029,  0.00731911,\n",
       "         0.0079971 ,  0.00846429,  0.0073612 ,  0.00946271,  0.00914535]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_func([np.ones(testData.X[0].shape)*0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predictions_func(data.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, (-19.807505, 0.30824822)), (5, (-19.453634, 0.30825487))]\n",
      "[(5, (-19.765078, 0.30825487)), (10, (-15.632283, 0.19002751))]\n",
      "[(5, (-19.546129, 0.30825487)), (12, (-17.958994, 0.24949609))]\n",
      "[(7, (-19.473919, 0.30306029)), (5, (-19.339251, 0.30825487))]\n",
      "[(5, (-19.665556, 0.30825487)), (17, (-16.285915, 0.19980519))]\n",
      "[(5, (-19.814039, 0.30825487)), (11, (-17.587646, 0.18616237))]\n",
      "[(19, (-20.242025, 0.30106205)), (5, (-19.573761, 0.30825487))]\n",
      "[(8, (-19.359825, 0.30875102)), (5, (-19.180332, 0.30825487))]\n",
      "[(9, (-19.739803, 0.29593748)), (5, (-19.521702, 0.30825487))]\n",
      "[(14, (-20.017479, 0.30824822)), (0, (-15.859643, 0.20394446))]\n",
      "[(14, (-19.884604, 0.30824822)), (12, (-18.430599, 0.24949609))]\n",
      "[(14, (-19.805882, 0.30824822)), (7, (-19.589598, 0.30306029))]\n",
      "[(14, (-20.625605, 0.30824822)), (17, (-16.758018, 0.19980519))]\n",
      "[(14, (-20.32843, 0.30824822)), (11, (-16.461458, 0.18616237))]\n",
      "[(19, (-19.894787, 0.30106205)), (14, (-19.87229, 0.30824822))]\n",
      "[(14, (-19.687641, 0.30824822)), (8, (-19.411964, 0.30875102))]\n",
      "[(14, (-19.480204, 0.30824822)), (9, (-19.268187, 0.29593748))]\n",
      "[(12, (-18.066683, 0.24949609)), (10, (-16.076612, 0.19002751))]\n",
      "[(7, (-19.804369, 0.30306029)), (10, (-15.85889, 0.19002751))]\n",
      "[(17, (-16.784769, 0.19980519)), (0, (-15.981397, 0.20394446))]\n",
      "[(11, (-17.062628, 0.18616237)), (10, (-16.202612, 0.19002751))]\n",
      "[(19, (-20.203003, 0.30106205)), (10, (-15.513548, 0.19002751))]\n",
      "[(8, (-19.632812, 0.30875102)), (10, (-15.927248, 0.19002751))]\n",
      "[(9, (-20.400848, 0.29593748)), (0, (-15.844625, 0.20394446))]\n",
      "[(7, (-19.934391, 0.30306029)), (12, (-18.08161, 0.24949609))]\n",
      "[(12, (-18.881891, 0.24949609)), (17, (-17.123238, 0.19980519))]\n",
      "[(12, (-18.139809, 0.24949609)), (11, (-16.681005, 0.18616237))]\n",
      "[(19, (-20.034388, 0.30106205)), (12, (-17.990248, 0.24949609))]\n",
      "[(8, (-19.910633, 0.30875102)), (12, (-17.983871, 0.24949609))]\n",
      "[(9, (-20.336979, 0.29593748)), (12, (-18.494102, 0.24949609))]\n",
      "[(7, (-20.607212, 0.30306029)), (17, (-16.268969, 0.19980519))]\n",
      "[(7, (-19.988876, 0.30306029)), (11, (-17.142265, 0.18616237))]\n",
      "[(19, (-19.707233, 0.30106205)), (7, (-19.221201, 0.30306029))]\n",
      "[(7, (-19.62545, 0.30306029)), (8, (-19.596508, 0.30875102))]\n",
      "[(7, (-19.505211, 0.30306029)), (9, (-19.44351, 0.29593748))]\n",
      "[(11, (-17.340336, 0.18616237)), (17, (-16.458563, 0.19980519))]\n",
      "[(19, (-20.552835, 0.30106205)), (17, (-16.663561, 0.19980519))]\n",
      "[(8, (-19.940727, 0.30875102)), (17, (-16.726795, 0.19980519))]\n",
      "[(9, (-19.972914, 0.29593748)), (17, (-16.453518, 0.19980519))]\n",
      "[(19, (-20.185291, 0.30106205)), (11, (-17.583744, 0.18616237))]\n",
      "[(8, (-19.912666, 0.30875102)), (11, (-16.401428, 0.18616237))]\n",
      "[(9, (-20.276495, 0.29593748)), (11, (-17.648685, 0.18616237))]\n",
      "[(19, (-19.870453, 0.30106205)), (8, (-19.501579, 0.30875102))]\n",
      "[(19, (-19.963694, 0.30106205)), (9, (-19.73288, 0.29593748))]\n",
      "[(9, (-19.836182, 0.29593748)), (8, (-19.66123, 0.30875102))]\n"
     ]
    }
   ],
   "source": [
    "for output_idx in range(data.Y.shape[-1]):\n",
    "    print(sorted(enumerate(zip(deeplift_model.get_layers()[-2].W[:,output_idx],\n",
    "                               deeplift_model.get_layers()[-4].b)), key=lambda x: -abs(x[1][0]) )[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node: 0\n",
      "[(10, -4.5617361), (11, -4.5606618), (12, -4.5579462), (13, -4.5523386), (14, -4.5269675)]\n",
      "node: 1\n",
      "[(21, 0.38230917), (20, 0.35195157), (49, 0.34952912), (22, 0.31976384), (23, 0.31735089)]\n",
      "node: 2\n",
      "[(7, 0.27965769), (6, 0.25055015), (9, 0.24158125), (44, 0.19970311), (29, 0.18103105)]\n",
      "node: 3\n",
      "[(25, -3.3250067), (28, -3.319689), (29, -3.3139145), (26, -3.3063612), (27, -3.3011355)]\n",
      "node: 4\n",
      "[(7, 0.25213218), (5, 0.24673554), (18, 0.18748948), (6, 0.18005429), (9, 0.16755639)]\n",
      "node: 5\n",
      "[(1, -5.764812), (3, -5.7621217), (0, -5.7486353), (4, -5.7479744), (2, -5.7422199)]\n",
      "node: 6\n",
      "[(27, -2.6288085), (26, -2.6136553), (29, -2.5932529), (28, -2.577121), (25, -2.5742512)]\n",
      "node: 7\n",
      "[(24, -5.7993035), (23, -5.795094), (22, -5.7878399), (21, -5.7834282), (20, -5.7756348)]\n",
      "node: 8\n",
      "[(40, -5.7506771), (44, -5.7505345), (41, -5.7465501), (43, -5.7456746), (42, -5.7349639)]\n",
      "node: 9\n",
      "[(49, -5.7643399), (46, -5.7599635), (48, -5.7537646), (45, -5.7519193), (47, -5.7518249)]\n",
      "node: 10\n",
      "[(14, -4.4802117), (11, -4.439847), (12, -4.4355502), (13, -4.4312778), (10, -4.4117155)]\n",
      "node: 11\n",
      "[(33, -4.6202321), (32, -4.5943022), (30, -4.5928121), (34, -4.5758572), (31, -4.566473)]\n",
      "node: 12\n",
      "[(15, -5.5442467), (17, -5.5224624), (19, -5.5129609), (18, -5.4970016), (16, -5.4903655)]\n",
      "node: 13\n",
      "[(10, 0.2572591), (2, 0.23517811), (38, 0.2022927), (0, 0.19563237), (47, 0.18646203)]\n",
      "node: 14\n",
      "[(8, -5.6778979), (9, -5.675312), (6, -5.6745081), (5, -5.6674838), (7, -5.6642146)]\n",
      "node: 15\n",
      "[(31, -4.1439147), (32, -4.1414385), (34, -4.1242247), (30, -4.1215472), (33, -4.1053271)]\n",
      "node: 16\n",
      "[(19, -2.7994258), (16, -2.7886672), (18, -2.7645781), (15, -2.7574551), (17, -2.7207198)]\n",
      "node: 17\n",
      "[(25, -5.0426965), (28, -5.0210476), (29, -5.0108142), (26, -5.0057917), (27, -4.9944887)]\n",
      "node: 18\n",
      "[(30, -2.0750353), (32, -2.0262864), (31, -2.0081658), (34, -1.9811856), (33, -1.9601573)]\n",
      "node: 19\n",
      "[(37, -5.6775413), (39, -5.6708355), (36, -5.6672478), (35, -5.6594381), (38, -5.6556549)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(deeplift_model.get_layers()[1].W.shape[1]):\n",
    "    print(\"node:\",i)\n",
    "    print(sorted(enumerate(deeplift_model.get_layers()[1].W[:,i]), key=lambda x: -np.abs(x[1]))[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the contributions for all 3 tasks and the multipliers for the third task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplift_contribs,\\\n",
    "deeplift_multipliers,\\\n",
    "grad_times_inp,\\\n",
    "grad,\\\n",
    "guided_backprop_times_inp,\\\n",
    "guided_backprop,\\\n",
    "guided_backprop_deeplift_times_inp,\\\n",
    "guided_backprop_deeplift,\\\n",
    "deconv_times_inp,\\\n",
    "deconv = [[np.array(contribs_func(task_idx=i, input_data_list=[data.X], batch_size=1000, progress_update=None))\n",
    "                    for i in range(data.Y.shape[-1])]\n",
    "                    for contribs_func in [deeplift_contribs_func,\n",
    "                                          deeplift_multipliers_func,\n",
    "                                          grad_times_inp_func,\n",
    "                                          grad_func,\n",
    "                                          guided_backprop_times_inp_func,\n",
    "                                          guided_backprop_func,\n",
    "                                          guided_backprop_deeplift_times_inp_func,\n",
    "                                          guided_backprop_deeplift_func,\n",
    "                                          deconv_times_inp_func,\n",
    "                                          deconv_func]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplift 0.9781520116\n",
      "grad_times_inp 0.8175276112\n",
      "grad 0.298641559752\n",
      "guided_backprop_times_inp 0.143816033779\n",
      "guided_backprop 0.0698287201052\n",
      "guided_backprop_deeplift_times_inp 0.144306206722\n",
      "guided_backprop_deeplift 0.0699108552511\n",
      "deconv_times_inp 0.091537127847\n",
      "deconv 0.0443167054198\n"
     ]
    }
   ],
   "source": [
    "scores = np.array(deeplift_contribs)\n",
    "for scores_name, scores in [('deeplift',deeplift_contribs),\n",
    "                            ('grad_times_inp', grad_times_inp),\n",
    "                            ('grad', grad),\n",
    "                            ('guided_backprop_times_inp', guided_backprop_times_inp),\n",
    "                            ('guided_backprop', guided_backprop),\n",
    "                            ('guided_backprop_deeplift_times_inp', guided_backprop_deeplift_times_inp),\n",
    "                            ('guided_backprop_deeplift', guided_backprop_deeplift),\n",
    "                            ('deconv_times_inp', deconv_times_inp),\n",
    "                            ('deconv', deconv)]:\n",
    "    num_caps = 10\n",
    "    inputs_per_cap = int(data.X.shape[-1]/num_caps)\n",
    "    task_idx = 0\n",
    "    true_positives = data.Y*(np.array(predictions)>0.5)\n",
    "    average_proportion_on_correct_array = []\n",
    "    for cap1 in range(num_caps):\n",
    "        for cap2 in range(cap1+1,num_caps):\n",
    "            relevant_inputs_cap1 = slice(cap1*inputs_per_cap,(cap1+1)*inputs_per_cap)\n",
    "            relevant_inputs_cap2 = slice(cap2*inputs_per_cap,(cap2+1)*inputs_per_cap)\n",
    "            true_positives_mask = true_positives[:,task_idx]\n",
    "            \n",
    "            scores_on_true_positives = np.compress(np.nonzero(true_positives_mask)[0],scores[task_idx],axis=0)\n",
    "            input_data_true_positives = np.compress(np.nonzero(true_positives_mask)[0], data.X, axis=0)\n",
    "            \n",
    "            scores_relevant_inputs_cap1 = scores_on_true_positives[:,relevant_inputs_cap1]\n",
    "            scores_relevant_inputs_cap2 = scores_on_true_positives[:,relevant_inputs_cap2]\n",
    "            signs_relevant_inputs_cap1 = np.sign(np.compress(np.nonzero(true_positives_mask)[0], data.X[:,relevant_inputs_cap1], axis=0))\n",
    "            signs_relevant_inputs_cap2 = np.sign(np.compress(np.nonzero(true_positives_mask)[0], data.X[:,relevant_inputs_cap2], axis=0))\n",
    "            correct_sign_scores_cap1 = np.abs(scores_relevant_inputs_cap1*(np.equal(np.sign(scores_relevant_inputs_cap1),\n",
    "                                                                                    signs_relevant_inputs_cap1)))\n",
    "            correct_sign_scores_cap2 = np.abs(scores_relevant_inputs_cap2*(np.equal(np.sign(scores_relevant_inputs_cap2),\n",
    "                                                                                    signs_relevant_inputs_cap2)))\n",
    "            \n",
    "            \n",
    "            total_scores = np.sum(np.abs(scores_on_true_positives), axis=1)\n",
    "            scores_on_relevant_indices = np.sum(correct_sign_scores_cap1, axis=1) +\\\n",
    "                                         np.sum(correct_sign_scores_cap2, axis=1)\n",
    "            total_scores = 0.0000001*(total_scores == 0) + total_scores #prevent div by zero\n",
    "            proportion_on_correct_indices = scores_on_relevant_indices/total_scores\n",
    "            average_proportion = np.mean(proportion_on_correct_indices)\n",
    "            average_proportion_on_correct_array.append(average_proportion)\n",
    "            task_idx += 1\n",
    "    print(scores_name, average_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
