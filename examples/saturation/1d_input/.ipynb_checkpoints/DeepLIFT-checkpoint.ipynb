{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General import statements. REMEMBER, DEEPLIFT_DIR needs to point to the deeplift directory WITHIN the deeplift repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division;\n",
    "from __future__ import print_function;\n",
    "from __future__ import absolute_import;\n",
    "import sys, os;\n",
    "from collections import OrderedDict, namedtuple;\n",
    "import numpy as np;\n",
    "\n",
    "#Make sure the directory is set to import the lab's version of keras\n",
    "scriptsDir = os.environ.get(\"KERAS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable KERAS_DIR\");\n",
    "sys.path.insert(0,scriptsDir)\n",
    "\n",
    "scriptsDir = os.environ.get(\"ENHANCER_SCRIPTS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable ENHANCER_SCRIPTS_DIR to point to the enhancer_prediction_code repo\");\n",
    "sys.path.insert(0,scriptsDir+\"/featureSelector/deepLIFFT\");\n",
    "from deepLIFTutils import makePngOfSequenceDeepLIFTScores\n",
    "\n",
    "import deeplift\n",
    "import deeplift.conversion.keras_conversion as kc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the keras model, make sure you normalise the weights\n",
    "#of the first convolutional layer to be mean-centered at each position.\n",
    "model_weights = \"modelsDir_runs/record_0_model_DXhpM_modelWeights.h5\"\n",
    "model_yaml = \"modelsDir_runs/record_0_model_DXhpM_modelYaml.yaml\"\n",
    "reload(kc)\n",
    "keras_model = kc.load_keras_model(model_weights, model_yaml, normalise_conv_for_one_hot_encoded_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'rows skipped from', 'features.gz')\n",
      "Returning desired dict\n",
      "('WARNING.', 90000, ' ids in the train/test/valid split files were not found in the input feature file. The first ten are: ', ['example79448', 'example79449', 'example79442', 'example79443', 'example79440', 'example79441', 'example79446', 'example79447', 'example79444', 'example79445'])\n",
      "Making numpy arrays out of the loaded files\n",
      "('train', 'shapeX', (8000, 50))\n",
      "('train', 'shapeY', (8000, 45))\n",
      "('valid', 'shapeX', (1000, 50))\n",
      "('valid', 'shapeY', (1000, 45))\n",
      "('test', 'shapeX', (1000, 50))\n",
      "('test', 'shapeY', (1000, 45))\n"
     ]
    }
   ],
   "source": [
    "scriptsDir = os.environ.get(\"UTIL_SCRIPTS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable UTIL_SCRIPTS_DIR to point to the deeplift code\");\n",
    "sys.path.insert(0,scriptsDir);\n",
    "from importDataPackage import importData\n",
    "reload(importData)\n",
    "trainData, validData, testData = importData.loadTrainTestValidFromYaml(\"yaml_10K/features.yaml\",\n",
    "                                                                       \"yaml_10K/labels.yaml\",\n",
    "                                                                       \"yaml_10K/splits.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = trainData.concat(validData, testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the keras sequential model into a deeplift sequential model, and compile the functions to compute the contributions and multipliers - the multipliers are analogous to the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'deeplift.conversion.keras_conversion' from '/Users/avantishrikumar/Research/deeplift/deeplift/conversion/keras_conversion.pyc'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deeplift.blobs import MxtsMode\n",
    "reload(kc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplift_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.DeepLIFT)\n",
    "deeplift_contribs_func = deeplift_model.get_target_contribs_func(find_scores_layer_idx=0)\n",
    "deeplift_multipliers_func = deeplift_model.get_target_multipliers_func(find_scores_layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deeplift_expo_upweight_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.DeepLIFT, expo_upweight_factor=1)\n",
    "deeplift_expo_upweight_contribs_func = deeplift_expo_upweight_model.get_target_contribs_func(find_scores_layer_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for other saliency map functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradients_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.Gradient)\n",
    "grad_times_inp_func = gradients_model.get_target_contribs_func(find_scores_layer_idx=0)\n",
    "guided_backprop_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.GuidedBackprop)\n",
    "guided_backprop_func = guided_backprop_model.get_target_contribs_func(find_scores_layer_idx=0)\n",
    "deconv_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.DeconvNet)\n",
    "deconv_func = deconv_model.get_target_contribs_func(find_scores_layer_idx=0)\n",
    "guided_backprop_deeplift1_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.GuidedBackpropDeepLIFT1)\n",
    "guided_backprop_deeplift1_func = guided_backprop_deeplift1_model.get_target_contribs_func(find_scores_layer_idx=0)\n",
    "guided_backprop_deeplift4_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.GuidedBackpropDeepLIFT4)\n",
    "guided_backprop_deeplift4_func = guided_backprop_deeplift4_model.get_target_contribs_func(find_scores_layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "predictions_func = theano.function([deeplift_model.get_layers()[0].get_activation_vars()],\n",
    "                                   deeplift_model.get_layers()[-1].get_activation_vars(),\n",
    "                                   allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02288487,  0.01503252,  0.01391376,  0.01689468,  0.01365434,\n",
       "         0.01088016,  0.01467786,  0.02019062,  0.01832591,  0.01921142,\n",
       "         0.01459569,  0.0175929 ,  0.01468958,  0.01762873,  0.01738314,\n",
       "         0.02023337,  0.01273123,  0.01259801,  0.01287257,  0.01017799,\n",
       "         0.01327073,  0.01009995,  0.01617088,  0.01522067,  0.01735248,\n",
       "         0.01058327,  0.0125776 ,  0.01211225,  0.01547582,  0.01146999,\n",
       "         0.0125803 ,  0.01609561,  0.01794443,  0.01786777,  0.01431722,\n",
       "         0.00875479,  0.00894387,  0.01631268,  0.01100512,  0.01454011,\n",
       "         0.02080688,  0.0154064 ,  0.01683253,  0.01237277,  0.0133016 ]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_func([np.ones(testData.X[0].shape)*0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predictions_func(data.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, (-9.90273, 0.42178848)), (5, (-8.8509884, 0.45459494))]\n",
      "[(10, (-8.8869429, 0.33364159)), (5, (-8.5870066, 0.45459494))]\n",
      "[(5, (-9.0000973, 0.45459494)), (16, (-7.4516282, 0.35848406))]\n",
      "[(7, (-10.08846, 0.4254503)), (5, (-8.6113901, 0.45459494))]\n",
      "[(5, (-8.7059097, 0.45459494)), (3, (-8.4875746, 0.30556861))]\n",
      "[(5, (-9.0618114, 0.45459494)), (18, (-8.620079, 0.33977368))]\n",
      "[(19, (-9.7191772, 0.39901626)), (5, (-8.8178024, 0.45459494))]\n",
      "[(8, (-9.4743862, 0.44665283)), (5, (-8.8355494, 0.45459494))]\n",
      "[(9, (-9.4154024, 0.44174463)), (5, (-8.5573454, 0.45459494))]\n",
      "[(14, (-10.185414, 0.42178848)), (10, (-8.2350903, 0.33364159))]\n",
      "[(14, (-10.235136, 0.42178848)), (16, (-7.2781382, 0.35848406))]\n",
      "[(14, (-10.490412, 0.42178848)), (7, (-10.199765, 0.4254503))]\n",
      "[(14, (-10.346035, 0.42178848)), (3, (-7.8338437, 0.30556861))]\n",
      "[(14, (-10.1131, 0.42178848)), (18, (-8.0485525, 0.33977368))]\n",
      "[(14, (-10.446726, 0.42178848)), (19, (-10.321595, 0.39901626))]\n",
      "[(14, (-10.086033, 0.42178848)), (8, (-9.0971603, 0.44665283))]\n",
      "[(14, (-10.456308, 0.42178848)), (9, (-10.216763, 0.44174463))]\n",
      "[(10, (-9.0297871, 0.33364159)), (16, (-8.1879301, 0.35848406))]\n",
      "[(7, (-10.507298, 0.4254503)), (10, (-8.8711386, 0.33364159))]\n",
      "[(10, (-9.138669, 0.33364159)), (3, (-8.1834087, 0.30556861))]\n",
      "[(10, (-8.7054911, 0.33364159)), (18, (-8.433629, 0.33977368))]\n",
      "[(19, (-10.231826, 0.39901626)), (10, (-8.6636448, 0.33364159))]\n",
      "[(8, (-9.6116762, 0.44665283)), (10, (-8.4619513, 0.33364159))]\n",
      "[(9, (-9.8422604, 0.44174463)), (10, (-8.1538944, 0.33364159))]\n",
      "[(7, (-10.264595, 0.4254503)), (16, (-7.9610562, 0.35848406))]\n",
      "[(3, (-9.2305946, 0.30556861)), (16, (-8.2499666, 0.35848406))]\n",
      "[(18, (-7.9299178, 0.33977368)), (16, (-7.48136, 0.35848406))]\n",
      "[(19, (-10.25454, 0.39901626)), (16, (-8.1683903, 0.35848406))]\n",
      "[(8, (-9.3876438, 0.44665283)), (16, (-7.6752415, 0.35848406))]\n",
      "[(9, (-10.179657, 0.44174463)), (16, (-7.2199354, 0.35848406))]\n",
      "[(7, (-10.284077, 0.4254503)), (3, (-9.0070076, 0.30556861))]\n",
      "[(7, (-10.163736, 0.4254503)), (18, (-8.1049051, 0.33977368))]\n",
      "[(7, (-10.254571, 0.4254503)), (19, (-9.9909468, 0.39901626))]\n",
      "[(7, (-10.202697, 0.4254503)), (8, (-9.53967, 0.44665283))]\n",
      "[(9, (-10.125635, 0.44174463)), (7, (-10.035287, 0.4254503))]\n",
      "[(3, (-8.6852303, 0.30556861)), (18, (-8.6112242, 0.33977368))]\n",
      "[(19, (-10.437792, 0.39901626)), (3, (-8.0977364, 0.30556861))]\n",
      "[(8, (-9.4699688, 0.44665283)), (3, (-7.9243379, 0.30556861))]\n",
      "[(9, (-10.21723, 0.44174463)), (3, (-8.3050394, 0.30556861))]\n",
      "[(19, (-10.312695, 0.39901626)), (18, (-8.0802059, 0.33977368))]\n",
      "[(8, (-9.229393, 0.44665283)), (18, (-7.7970304, 0.33977368))]\n",
      "[(9, (-9.868021, 0.44174463)), (18, (-8.4380016, 0.33977368))]\n",
      "[(19, (-10.017264, 0.39901626)), (8, (-9.651165, 0.44665283))]\n",
      "[(19, (-10.171752, 0.39901626)), (9, (-10.097359, 0.44174463))]\n",
      "[(9, (-10.008742, 0.44174463)), (8, (-9.6246853, 0.44665283))]\n"
     ]
    }
   ],
   "source": [
    "for output_idx in range(data.Y.shape[-1]):\n",
    "    print(sorted(enumerate(zip(deeplift_model.get_layers()[-2].W[:,output_idx],\n",
    "                               deeplift_model.get_layers()[-4].b)), key=lambda x: -abs(x[1][0]) )[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node: 0\n",
      "[(14, -3.0823469), (12, -3.066566), (13, -3.0520539), (11, -3.0448103), (10, -2.9601963)]\n",
      "node: 1\n",
      "[(2, 0.69218224), (0, 0.57025635), (48, 0.56648636), (1, 0.56131423), (3, 0.54749012)]\n",
      "node: 2\n",
      "[(1, -1.145311), (3, -1.0972462), (4, -1.0561395), (2, -0.99623019), (0, -0.98296332)]\n",
      "node: 3\n",
      "[(26, -3.5051348), (28, -3.5043664), (29, -3.4993424), (27, -3.4882243), (25, -3.4365299)]\n",
      "node: 4\n",
      "[(18, 0.70912421), (16, 0.65234143), (19, 0.59847224), (17, 0.59531063), (15, 0.52341205)]\n",
      "node: 5\n",
      "[(1, -4.1212611), (3, -4.0934711), (4, -4.082191), (2, -4.0742764), (0, -4.0715995)]\n",
      "node: 6\n",
      "[(4, -1.0874352), (0, -1.0732213), (3, -0.95294684), (41, -0.88277102), (1, -0.8556034)]\n",
      "node: 7\n",
      "[(24, -4.2877216), (23, -4.2757592), (21, -4.2328453), (22, -4.2306466), (20, -4.2299652)]\n",
      "node: 8\n",
      "[(44, -4.1709027), (43, -4.1180568), (42, -4.1108727), (40, -4.0983443), (41, -4.0622697)]\n",
      "node: 9\n",
      "[(49, -4.1456556), (46, -4.1388416), (48, -4.100719), (47, -4.0803556), (45, -4.0795326)]\n",
      "node: 10\n",
      "[(10, -3.8386698), (13, -3.801806), (12, -3.7978487), (11, -3.7775302), (14, -3.7659185)]\n",
      "node: 11\n",
      "[(36, -1.9195702), (39, -1.8167096), (38, -1.7318425), (37, -1.667231), (35, -1.6359813)]\n",
      "node: 12\n",
      "[(16, -3.0022299), (17, -2.9961181), (19, -2.9729767), (18, -2.9522557), (15, -2.888737)]\n",
      "node: 13\n",
      "[(4, -0.69714332), (3, -0.67147672), (1, -0.60158372), (32, 0.53947288), (42, -0.48376837)]\n",
      "node: 14\n",
      "[(8, -4.1445084), (9, -4.122016), (5, -4.1216583), (7, -4.1008453), (6, -4.0972471)]\n",
      "node: 15\n",
      "[(34, -3.0825908), (33, -3.0753365), (31, -3.0557239), (30, -3.0538292), (32, -3.0321293)]\n",
      "node: 16\n",
      "[(18, -3.643225), (15, -3.6302664), (16, -3.6200795), (19, -3.615109), (17, -3.610595)]\n",
      "node: 17\n",
      "[(29, -3.3975844), (26, -3.3921757), (28, -3.3810675), (25, -3.3587689), (27, -3.3436105)]\n",
      "node: 18\n",
      "[(31, -3.8462839), (30, -3.8091278), (33, -3.802772), (34, -3.800395), (32, -3.7995546)]\n",
      "node: 19\n",
      "[(37, -4.0956521), (35, -4.0468292), (39, -4.0450878), (36, -4.0325303), (38, -4.0179696)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(deeplift_model.get_layers()[1].W.shape[1]):\n",
    "    print(\"node:\",i)\n",
    "    print(sorted(enumerate(deeplift_model.get_layers()[1].W[:,i]), key=lambda x: -np.abs(x[1]))[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the contributions for all 3 tasks and the multipliers for the third task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplift_contribs,\\\n",
    "deeplift_expo_upweight_contribs,\\\n",
    "deeplift_multipliers,\\\n",
    "grad_times_inp,\\\n",
    "guided_backprop,\\\n",
    "deconv_contribs = [[np.array(contribs_func(task_idx=i, input_data_list=[data.X], batch_size=1000, progress_update=None))\n",
    "                    for i in range(data.Y.shape[-1])]\n",
    "                    for contribs_func in [deeplift_contribs_func,\n",
    "                                          deeplift_expo_upweight_contribs_func,\n",
    "                                          deeplift_multipliers_func,\n",
    "                                          grad_times_inp_func,\n",
    "                                          guided_backprop_func,\n",
    "                                          deconv_func]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplift 0.958514390337\n",
      "deeplift_expo_upweight 0.946541609601\n",
      "grad_times_inp 0.85333035419\n",
      "guided_backprop 0.26892951981\n",
      "deconv 0.211597196932\n"
     ]
    }
   ],
   "source": [
    "scores = np.array(deeplift_contribs)\n",
    "for scores_name, scores in [('deeplift',deeplift_contribs),\n",
    "                            ('deeplift_expo_upweight', deeplift_expo_upweight_contribs),\n",
    "                            ('grad_times_inp', grad_times_inp),\n",
    "                            ('guided_backprop', guided_backprop),\n",
    "                            ('deconv', deconv_contribs)]:\n",
    "    num_caps = 10\n",
    "    inputs_per_cap = int(data.X.shape[-1]/num_caps)\n",
    "    task_idx=0\n",
    "    true_positives = data.Y*(np.array(predictions)>0.5)\n",
    "    average_proportion_on_correct_array = []\n",
    "    for cap1 in range(num_caps):\n",
    "        for cap2 in range(cap1+1,num_caps):\n",
    "            relevant_inputs_cap1 = slice(cap1*inputs_per_cap,(cap1+1)*inputs_per_cap)\n",
    "            relevant_inputs_cap2 = slice(cap2*inputs_per_cap,(cap2+1)*inputs_per_cap)\n",
    "            true_positives_mask = true_positives[:,task_idx]\n",
    "            scores_on_true_positives = np.abs(np.compress(np.nonzero(true_positives_mask)[0],scores[task_idx],axis=0))\n",
    "            total_scores = np.sum(scores_on_true_positives, axis=1)\n",
    "            total_scores = 0.0000001*(total_scores == 0) + total_scores\n",
    "            scores_on_relevant_indices = np.sum(scores_on_true_positives[:,relevant_inputs_cap1], axis=1) +\\\n",
    "                                         np.sum(scores_on_true_positives[:,relevant_inputs_cap2], axis=1)\n",
    "            proportion_on_correct_indices = scores_on_relevant_indices/total_scores\n",
    "            average_proportion = np.mean(proportion_on_correct_indices)\n",
    "            average_proportion_on_correct_array.append(average_proportion)\n",
    "            task_idx += 1\n",
    "    print(scores_name, average_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
