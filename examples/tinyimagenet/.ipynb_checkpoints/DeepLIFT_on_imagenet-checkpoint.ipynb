{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 6: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5004)\n",
      "Using Theano backend.\n",
      "/users/avanti/anaconda2/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "#Set up environment and import necessary dependencies\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import division;\n",
    "from __future__ import print_function;\n",
    "from __future__ import absolute_import;\n",
    "import sys, os;\n",
    "from collections import OrderedDict, namedtuple, Counter;\n",
    "import numpy as np;\n",
    "import theano\n",
    "np.random.seed(1234)\n",
    "\n",
    "scriptsDir = os.environ.get(\"KERAS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable KERAS_DIR\");\n",
    "sys.path.insert(0,scriptsDir)\n",
    "import keras\n",
    "\n",
    "scriptsDir = os.environ.get(\"DEEPLIFT_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable DEEPLIFT_DIR\");\n",
    "sys.path.insert(0,scriptsDir)\n",
    "\n",
    "#Import some general util stuff\n",
    "scriptsDir = os.environ.get(\"UTIL_SCRIPTS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable UTIL_SCRIPTS_DIR to point to av_scripts\");\n",
    "sys.path.insert(0,scriptsDir);\n",
    "import pathSetter;\n",
    "import util;\n",
    "import fileProcessing as fp\n",
    "from plottingUtilitiesPackage import matplotlibHelpers as mplh;\n",
    "\n",
    "scriptsDir = os.environ.get(\"ENHANCER_SCRIPTS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable ENHANCER_SCRIPTS_DIR to point to enhancer_prediction_code\");\n",
    "sys.path.insert(0,scriptsDir+\"/featureSelector/deepLIFFT/\");\n",
    "import criticalSubsetIdentification as csi\n",
    "#import deepLIFT stuff\n",
    "import deepLIFTutils\n",
    "sys.path.insert(0,scriptsDir+\"/featureSelector/deepLIFFT/kerasBasedBackprop\");\n",
    "from deepLIFTonGPU import ScoreTypes, Activations_enum, OutLayerInfo, getScoreFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the keras model, make sure you normalise the weights\n",
    "#of the first convolutional layer to be mean-centered at each position.\n",
    "import vgg16_keras \n",
    "reload(vgg16_keras)\n",
    "from vgg16_keras import *\n",
    "import h5py\n",
    "reload(deepLIFTutils)\n",
    "path_to_weights_file = '/srv/scratch/annashch/cs231n_project/vgg16_weights_dropout_regularization_augmenteddataTintContrast.stupidsgd.hdf5'\n",
    "keras_model=VGG_16(path_to_weights_file)\n",
    "#mean_normalise_softmax_weights(keras_model.layers[-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras_conversion as kc\n",
    "reload(kc)\n",
    "import deeplift_backend as B\n",
    "reload(B)\n",
    "import blobs\n",
    "reload(blobs)\n",
    "import models\n",
    "reload(models)\n",
    "import deeplift_util\n",
    "reload(deeplift_util)\n",
    "from blobs import MxtsMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean-normalising softmax\n"
     ]
    }
   ],
   "source": [
    "deeplift_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.DeepLIFT)\n",
    "deeplift_contribs_func = deeplift_model.get_target_contribs_func(input_layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean-normalising softmax\n"
     ]
    }
   ],
   "source": [
    "guided_backprop_deeplift_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.GuidedBackpropDeepLIFT)\n",
    "guided_backprop_deeplift_contribs_func = guided_backprop_deeplift_model.get_target_multipliers_func(input_layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean-normalising softmax\n"
     ]
    }
   ],
   "source": [
    "gradient_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.Gradient)\n",
    "gradient_func = gradient_model.get_target_multipliers_func(input_layer_idx=0)\n",
    "grad_times_inp_func = gradient_model.get_target_contribs_func(input_layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean-normalising softmax\n"
     ]
    }
   ],
   "source": [
    "deconv_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.DeconvNet)\n",
    "deconv_contribs_func = deconv_model.get_target_multipliers_func(input_layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean-normalising softmax\n"
     ]
    }
   ],
   "source": [
    "guided_backprop_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.GuidedBackprop)\n",
    "guided_backprop_contribs_func = guided_backprop_model.get_target_multipliers_func(input_layer_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "path_to_data_file = '/srv/scratch/annashch/tiny-imagenet-200/imagenet.transpose.individually.augment.hdf5'#/srv/scratch/annashch/cs231n_project/imagenet.hdf5'\n",
    "data=h5py.File(path_to_data_file,'r')\n",
    "xData=data['X_valid']\n",
    "yData=np.array(data['Y_valid'])\n",
    "print(len(yData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deeplift_output_func = theano.function([deeplift_model.get_layers()[0].get_activation_vars()],\n",
    "                                        deeplift_model.get_layers()[-1].get_activation_vars(),\n",
    "                                        allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get indices of correct predictions\n",
    "deeplift_outputs = deeplift_util.run_function_in_batches(deeplift_output_func, input_data_list=[xData])\n",
    "deeplift_outputs = np.array(deeplift_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_predictions = [(i,np.max(deeplift_outputs[i]))\n",
    "                           for i in xrange(len(yData))\n",
    "                           if np.argmax(yData[i]) == np.argmax(deeplift_outputs[i])]\n",
    "print(len(correct_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction_lookup = dict([x for x in correct_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_output_file = \"predictions_summary.txt\"\n",
    "fp.writeToFile(predictions_output_file, \"\\n\".join([str(i)+\"\\t\"+str(deeplift_outputs[i][np.argmax(yData[i])])\n",
    "                                                         +\"\\t\"+str(i in correct_prediction_lookup)\n",
    "                                                         +\"\\t\"+str(np.argmax(yData[i]))\n",
    "                                                         +\"\\t\"+\",\".join([str(x[0]) for x in sorted(enumerate(deeplift_outputs[i]), key=lambda x:-x[1])[:10]])\n",
    "                                                       for i in xrange(len(deeplift_outputs))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the outputs of the net on the validation set\n",
    "#reload(deepLIFTutils)\n",
    "#print(np.shape(data))\n",
    "#import theano\n",
    "#import deeplift_util\n",
    "#\n",
    "#keras_output_func = theano.function([keras_model.layers[0].input],\n",
    "#                                     keras_model.layers[-1].get_output(train=False),\n",
    "#                                     allow_input_downcast=True)\n",
    "#keras_outputs = deeplift_util.run_function_in_batches(keras_output_func, input_data_list=[data])\n",
    "#keras_outputs=np.array(keras_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#deeplift_output_func = theano.function([deeplift_model.get_layers()[0].get_activation_vars()],\n",
    "#                                        deeplift_model.get_layers()[-1].get_activation_vars(),\n",
    "#                                        allow_input_downcast=True)\n",
    "#deeplift_outputs = deeplift_util.run_function_in_batches(deeplift_output_func, input_data_list=[data])\n",
    "#deeplift_outputs = np.array(deeplift_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.sum(np.abs(keras_outputs-deeplift_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale_func_for_image(values):\n",
    "    min_val = 0\n",
    "    values = (255.0 - min_val)*(values - np.min(values))/(np.max(values)-np.min(values))  + min_val\n",
    "    return np.transpose(np.array([values]*3), axes=(1,2,0))#np.sum(values,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "def save_image(values, output_dir, suffix, idx):\n",
    "    values = rescale_func(values).astype(np.uint8)\n",
    "    name = output_dir+\"/val_\"+str(idx)+\"_\"+suffix+\".PNG\"\n",
    "    scipy.misc.imsave(name, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveSequentialToHDF5\n",
    "    import h5py;                                                                \n",
    "    f=h5py.File(options.saveToHDF5,'w')                                         \n",
    "    dsetNames=['train','valid','test','eval']                                   \n",
    "    dsetVals=[trainData,validData,testData,evalData]                            \n",
    "    for i in range(len(dsetNames)):                                             \n",
    "        if dsetVals[i]==None:                                                   \n",
    "            continue #case when eval set is not used                            \n",
    "        dsetName=dsetNames[i]                                                   \n",
    "        dsetVal=dsetVals[i]                                                     \n",
    "        dset_Ids=f.create_dataset(dsetName+\"Id\",data=dsetVal.ids)               \n",
    "        dset_X=f.create_dataset(dsetName+\"X\",data=dsetVal.X)                    \n",
    "        dset_Y=f.create_dataset(dsetName+\"Y\",data=dsetVal.Y)                    \n",
    "        dset_LabelNames=f.create_dataset(dsetName+\"LabelNames\",data=dsetVal.labelNames)\n",
    "        if dsetVal.featureNames!=None:                                          \n",
    "            dset_FeatureNames=f.create_dataset(dsetName+\"FeatureNames\",data=dsetVal.featureNames)\n",
    "        if dsetVal.weights!=None:                                               \n",
    "            dset_Weights=f.create_dataset(dsetName+\"Weights\",data=dsetVal.weights)\n",
    "    f.flush()                                                                   \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flush_class(class_to_datapoints, class_to_datapoint_indices, the_class):\n",
    "    datapoints = class_to_datapoints[the_class]\n",
    "    indices = class_to_datapoint_indices[the_class]\n",
    "    class_to_datapoints[the_class] = []\n",
    "    class_to_datapoint_indices[the_class] = []\n",
    "    deeplift_contribs = target_contribs_func(task_idx=the_class,\n",
    "                                             input_data_list=[datapoints],\n",
    "                                             batch_size=10,\n",
    "                                             progress_update=None)\n",
    "    gradients, grad_times_inp_contribs, deeplift_contribs =\\\n",
    "                                                [contribs_func(task_idx=the_class,\n",
    "                                                         input_data_list=[datapoints],\n",
    "                                                         batch_size=10,\n",
    "                                                         progress_update=None) for\n",
    "                                                           contribs_func in [gradient_func,\n",
    "                                                                             grad_times_inp_func,\n",
    "                                                                             deeplift_contribs_func]]\n",
    "    for j in xrange(len(gradients)):\n",
    "        simonyan = np.abs(gradients[j])\n",
    "        simonyan_blackwhite = np.linalg.norm(simonyan, axis=0)\n",
    "        \n",
    "        grad_x_inp = (gradients[j]*datapoints[j])*(gradients[j]*datapoints[j])\n",
    "        grad_x_inp_blackwhite = np.sum(grad_x_inp > 0, axis=0)\n",
    "        deeplift_contr = deeplift_contribs[j]\n",
    "        deeplift_contribs_blackwhite = np.sum(deeplift_contr > 0, axis=0)\n",
    "        \n",
    "        for (image, image_name) in [(simonyan_blackwhite, \"simonyan\"),\n",
    "                                    (grad_x_inp_blackwhite, \"grad-x-inp\"),\n",
    "                                    (deeplift_contribs_blackwhite, \"liftpad\")]:\n",
    "            save_image(image, output_dir, image_name, indices[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "read_data_batch_size = 100\n",
    "class_flush_threshold = 10\n",
    "output_dir=\"saliency_maps\"\n",
    "num_classes=200\n",
    "\n",
    "class_to_datapoints = dict([(x,[]) for x in xrange(num_classes)])\n",
    "class_to_datapoint_indices = dict([(x,[]) for x in xrange(num_classes)])\n",
    "while i < len(xData):\n",
    "    if (i%1000 == 0):\n",
    "        print(\"Done \",i)\n",
    "    xDataInMem = np.array(xData[i:i+read_data_batch_size])\n",
    "    for j in xrange(len(xDataInMem)):\n",
    "        the_class = np.argmax(yData[i+j])\n",
    "        class_to_datapoints[the_class].append(xDataInMem[j])\n",
    "        class_to_datapoint_indices[the_class].append(i+j)\n",
    "        if (len(class_to_datapoints[the_class]) > class_flush_threshold):\n",
    "            flush_class(class_to_datapoints, class_to_datapoint_indices, the_class)\n",
    "    i += read_data_batch_size\n",
    "\n",
    "for the_class in xrange(num_classes):\n",
    "    flush_class(class_to_datapoints, class_to_datapoint_indices, the_class)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale_func(values, squish=False):\n",
    "    values = np.sum(values,axis=0)\n",
    "    if (squish):\n",
    "        values = np.log(1+values)\n",
    "    min_val = 40\n",
    "    values = (255.0 - min_val)*(values - np.min(values))/(np.max(values)-np.min(values))  + min_val\n",
    "    return (np.transpose(np.array([values]*3), axes=(1,2,0))).astype(np.uint8)#np.sum(values,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 117\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "left cannot be >= right",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-01139fef23ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_plots\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnum_plots\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Original'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/avanti/anaconda2/lib/python2.7/site-packages/matplotlib/figure.pyc\u001b[0m in \u001b[0;36msubplots_adjust\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m         \"\"\"\n\u001b[1;32m-> 1586\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplotpars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSubplotBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/users/avanti/anaconda2/lib/python2.7/site-packages/matplotlib/figure.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, left, bottom, right, top, wspace, hspace)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'left cannot be >= right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbottom\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: left cannot be >= right"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa5d59a63d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import skimage \n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "indices = [18, 5, 8, 24, 17]\n",
    "#indices = range(20,30)\n",
    "fig = plt.figure(figsize=(20, 4*len(indices)), facecolor='w')\n",
    "num_plots=7\n",
    "for i,idx in enumerate(indices):\n",
    "    softmax_class = np.argmax(yData[idx])\n",
    "    print(\"class:\",softmax_class)\n",
    "    #compute gradients using the model\n",
    "    gradients,\\\n",
    "    grad_times_inp_contribs,\\\n",
    "    deconv_contribs,\\\n",
    "    guided_backprop_contribs,\\\n",
    "    deeplift_contribs,\\\n",
    "    guided_backprop_deeplift_contribs = [contribs_func(task_idx=softmax_class,\n",
    "                         input_data_list=[[xData[idx]]],\n",
    "                         batch_size=10,\n",
    "                         progress_update=None)[0] for\n",
    "                           contribs_func in [gradient_func,\n",
    "                                             grad_times_inp_func,\n",
    "                                             deconv_contribs_func,\n",
    "                                             guided_backprop_contribs_func,\n",
    "                                             deeplift_contribs_func,\n",
    "                                             guided_backprop_deeplift_contribs_func]]\n",
    "    img_original=imread('images/val_'+str(idx)+'.JPEG').astype(np.uint8)\n",
    "    fig.subplots_adjust(wspace=0.05)\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    #fig.subplots_adjust(right=0)\n",
    "    plt.subplot(len(indices), num_plots, i*num_plots + 1)\n",
    "    if (i==1):\n",
    "        plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img_original)\n",
    "\n",
    "    plt.subplot(len(indices), num_plots, i*num_plots + 2)\n",
    "    if (i==1):\n",
    "        plt.title('Deconv')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(rescale_func(deconv_contribs*(deconv_contribs>0)))\n",
    "    \n",
    "    plt.subplot(len(indices), num_plots, i*num_plots + 3)\n",
    "    if (i==1):\n",
    "        plt.title('Abs. gradients')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(rescale_func(np.abs(gradients)))\n",
    "\n",
    "    plt.subplot(len(indices), num_plots, i*num_plots + 4)\n",
    "    if (i==1):\n",
    "        plt.title('Taylor')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(rescale_func(grad_times_inp_contribs*((grad_times_inp_contribs) > 0)))\n",
    "\n",
    "    plt.subplot(len(indices),num_plots, i*num_plots + 5)\n",
    "    if (i==1):\n",
    "        plt.title('DeepLIFT')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(rescale_func(deeplift_contribs*(deeplift_contribs>0), squish=True))\n",
    "    \n",
    "    plt.subplot(len(indices), num_plots, i*num_plots + 6)\n",
    "    if (i==1):\n",
    "        plt.title('Guided Backprop')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(rescale_func(guided_backprop_contribs*(guided_backprop_contribs>0)))\n",
    "    #plt.imshow(rescale_func(guided_backprop_contribs))\n",
    "\n",
    "    plt.subplot(len(indices), num_plots, i*num_plots + 7)\n",
    "    if (i==1):\n",
    "        plt.title('Guided Backprop DL')\n",
    "    plt.axis('off')\n",
    "    plt.imshow(rescale_func(guided_backprop_deeplift_contribs*(guided_backprop_deeplift_contribs>0)))\n",
    "    #plt.imshow(rescale_func(guided_backprop_deeplift_contribs))\n",
    "    \n",
    "    #plt.subplot(len(indices),num_plots, i*num_plots + 7)\n",
    "    #plt.title('Guided Backprop Taylor')\n",
    "    #plt.axis('off')\n",
    "    #gpt = guided_backprop_contribs*xData[idx]\n",
    "    #plt.imshow(rescale_func(gpt*(gpt>0)))\n",
    "    \n",
    "    #plt.subplot(len(indices),num_plots, i*num_plots + 8)\n",
    "    #plt.title('Guided Backprop DeepLIFT')\n",
    "    #plt.axis('off')\n",
    "    #gpdl = guided_backprop_deeplift_contribs*xData[idx]\n",
    "    #plt.imshow(rescale_func(gpdl*(gpdl>0)))\n",
    "    \n",
    "    #plt.subplot(len(indices),num_plots, i*num_plots + 9)\n",
    "    #plt.title('difference')\n",
    "    #plt.axis('off')\n",
    "    #diff = gpdl-gpt\n",
    "    #plt.imshow(rescale_func(diff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
