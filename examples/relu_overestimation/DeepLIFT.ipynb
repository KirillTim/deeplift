{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General import statements. REMEMBER, DEEPLIFT_DIR needs to point to the deeplift directory WITHIN the deeplift repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division;\n",
    "from __future__ import print_function;\n",
    "from __future__ import absolute_import;\n",
    "import sys, os;\n",
    "from collections import OrderedDict, namedtuple;\n",
    "import numpy as np;\n",
    "\n",
    "#Make sure the directory is set to import the lab's version of keras\n",
    "scriptsDir = os.environ.get(\"KERAS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable KERAS_DIR\");\n",
    "sys.path.insert(0,scriptsDir)\n",
    "\n",
    "scriptsDir = os.environ.get(\"ENHANCER_SCRIPTS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable ENHANCER_SCRIPTS_DIR to point to the enhancer_prediction_code repo\");\n",
    "sys.path.insert(0,scriptsDir+\"/featureSelector/deepLIFFT\");\n",
    "from deepLIFTutils import makePngOfSequenceDeepLIFTScores\n",
    "\n",
    "import deeplift\n",
    "import deeplift.conversion.keras_conversion as kc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the keras model, make sure you normalise the weights\n",
    "#of the first convolutional layer to be mean-centered at each position.\n",
    "model_weights = \"modelsDir_runs/record_0_model_9CS6P_modelWeights.h5\"\n",
    "model_yaml = \"modelsDir_runs/record_0_model_9CS6P_modelYaml.yaml\"\n",
    "reload(kc)\n",
    "keras_model = kc.load_keras_model(model_weights, model_yaml, normalise_conv_for_one_hot_encoded_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'rows skipped from', 'features.gz')\n",
      "Returning desired dict\n",
      "Making numpy arrays out of the loaded files\n",
      "('train', 'shapeX', (80000, 20))\n",
      "('train', 'shapeY', (80000, 1))\n",
      "('valid', 'shapeX', (10000, 20))\n",
      "('valid', 'shapeY', (10000, 1))\n",
      "('test', 'shapeX', (10000, 20))\n",
      "('test', 'shapeY', (10000, 1))\n"
     ]
    }
   ],
   "source": [
    "scriptsDir = os.environ.get(\"UTIL_SCRIPTS_DIR\");\n",
    "if (scriptsDir is None):\n",
    "    raise Exception(\"Please set environment variable UTIL_SCRIPTS_DIR to point to the deeplift code\");\n",
    "sys.path.insert(0,scriptsDir);\n",
    "from importDataPackage import importData\n",
    "reload(importData)\n",
    "trainData, validData, testData = importData.loadTrainTestValidFromYaml(\"yaml/features.yaml\",\n",
    "                                                                       \"yaml/labels.yaml\",\n",
    "                                                                       \"yaml/splits.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = trainData.concat(validData, testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the keras sequential model into a deeplift sequential model, and compile the functions to compute the contributions and multipliers - the multipliers are analogous to the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'deeplift.conversion.keras_conversion' from '/Users/avantishrikumar/Research/deeplift/deeplift/conversion/keras_conversion.pyc'>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deeplift.blobs import MxtsMode\n",
    "reload(kc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from deeplift import blobs\n",
    "reload(blobs)\n",
    "deeplift_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.DeepLIFT)\n",
    "deeplift_contribs_func = deeplift_model.get_target_contribs_func(find_scores_layer_idx=0, target_layer_idx=-1)\n",
    "deeplift_multipliers_func = deeplift_model.get_target_multipliers_func(find_scores_layer_idx=0, target_layer_idx=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for other saliency map functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradients_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.Gradient)\n",
    "grad_times_inp_func = gradients_model.get_target_contribs_func(find_scores_layer_idx=0, target_layer_idx=-1)\n",
    "grad_func = gradients_model.get_target_multipliers_func(find_scores_layer_idx=0, target_layer_idx=-1)\n",
    "guided_backprop_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.GuidedBackprop)\n",
    "guided_backprop_times_inp_func = guided_backprop_model.get_target_contribs_func(find_scores_layer_idx=0, target_layer_idx=-1)\n",
    "guided_backprop_func = guided_backprop_model.get_target_multipliers_func(find_scores_layer_idx=0, target_layer_idx=-1)\n",
    "deconv_model = kc.convert_sequential_model(keras_model, mxts_mode=MxtsMode.DeconvNet)\n",
    "deconv_func = deconv_model.get_target_multipliers_func(find_scores_layer_idx=0, target_layer_idx=-1)\n",
    "deconv_times_inp_func = deconv_model.get_target_contribs_func(find_scores_layer_idx=0, target_layer_idx=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "predictions_func = theano.function([deeplift_model.get_layers()[0].get_activation_vars()],\n",
    "                                   deeplift_model.get_layers()[-1].get_activation_vars(),\n",
    "                                   allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32562831]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_func([np.ones(testData.X[0].shape)*0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predictions_func(data.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the contributions for all 3 tasks and the multipliers for the third task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplift_contribs,\\\n",
    "deeplift_multipliers,\\\n",
    "grad_times_inp,\\\n",
    "grad,\\\n",
    "guided_backprop_times_inp,\\\n",
    "guided_backprop,\\\n",
    "deconv_times_inp,\\\n",
    "deconv = [np.array(contribs_func(task_idx=0, input_data_list=[data.X], batch_size=1000, progress_update=None))\n",
    "                    for contribs_func in [deeplift_contribs_func,\n",
    "                                          deeplift_multipliers_func,\n",
    "                                          grad_func,\n",
    "                                          grad_times_inp_func,\n",
    "                                          guided_backprop_times_inp_func,\n",
    "                                          guided_backprop_func,\n",
    "                                          deconv_times_inp_func,\n",
    "                                          deconv_func]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives:  (20000,) 20000\n",
      "deeplift 0.00147205033174\n",
      "grad_times_inp 0.259726147902\n",
      "grad 0.0798918614418\n",
      "guided_backprop_times_inp 0.0872208890279\n",
      "guided_backprop 0.229953927246\n",
      "deconv_times_inp 0.151141441722\n",
      "deconv 0.500190961702\n"
     ]
    }
   ],
   "source": [
    "true_positives = np.abs(data.Y[:,0] - np.array(predictions[:,0])) < 0.2\n",
    "print(\"True positives: \", true_positives.shape, np.sum(true_positives))\n",
    "thresholds = np.arange(0,1,1.0/data.X.shape[-1])\n",
    "true_scores = np.maximum(data.X-thresholds[None,:],0)\n",
    "\n",
    "for scores_name, scores in [('deeplift',deeplift_contribs),\n",
    "                            ('grad_times_inp', grad_times_inp),\n",
    "                            ('grad', grad),\n",
    "                            ('guided_backprop_times_inp', guided_backprop_times_inp),\n",
    "                            ('guided_backprop', guided_backprop),\n",
    "                            ('deconv_times_inp', deconv_times_inp),\n",
    "                            ('deconv', deconv)]:\n",
    "    scores = scores\n",
    "    #normalise scores to equal to prediction\n",
    "    #scores = scores * (predictions[:,0]/np.sum(scores, axis=1))[:,None]\n",
    "    \n",
    "    true_scores_on_true_positives = np.compress(np.nonzero(true_positives.squeeze())[0], true_scores, axis=0)\n",
    "    scores_on_true_positives = np.compress(np.nonzero(true_positives.squeeze())[0], scores, axis=0)\n",
    "    mse = np.mean(np.square(true_scores_on_true_positives-scores_on_true_positives))\n",
    "    print(scores_name, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
