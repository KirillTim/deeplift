%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Avanti Shrikumar at 2016-05-08 14:03:48 -0700 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Long2015-kw,
	Author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	Booktitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	Pages = {3431--3440},
	Title = {Fully convolutional networks for semantic segmentation},
	Year = 2015}

@misc{Chollet2015-ya,
	Author = {Chollet, Francois},
	Date-Modified = {2016-05-08 21:03:45 +0000},
	Title = {Keras},
	Year = 2015}

@incollection{Zeiler2014-sk,
	Author = {Zeiler, Matthew D and Fergus, Rob},
	Booktitle = {Computer {vision--ECCV} 2014},
	Pages = {818--833},
	Publisher = {Springer},
	Title = {Visualizing and understanding convolutional networks},
	Year = 2014}

@article{Samek2015-oc,
	Abstract = {Deep Neural Networks (DNNs) have demonstrated impressive
                   performance in complex machine learning tasks such as image
                   classification or speech recognition. However, due to their
                   multi-layer nonlinear structure, they are not transparent,
                   i.e., it is hard to grasp what makes them arrive at a
                   particular classification or recognition decision given a
                   new unseen data sample. Recently, several approaches have
                   been proposed enabling one to understand and interpret the
                   reasoning embodied in a DNN for a single test image. These
                   methods quantify the ''importance'' of individual pixels wrt
                   the classification decision and allow a visualization in
                   terms of a heatmap in pixel/input space. While the
                   usefulness of heatmaps can be judged subjectively by a
                   human, an objective quality measure is missing. In this
                   paper we present a general methodology based on region
                   perturbation for evaluating ordered collections of pixels
                   such as heatmaps. We compare heatmaps computed by three
                   different methods on the SUN397, ILSVRC2012 and MIT Places
                   data sets. Our main result is that the recently proposed
                   Layer-wise Relevance Propagation (LRP) algorithm
                   qualitatively and quantitatively provides a better
                   explanation of what made a DNN arrive at a particular
                   classification decision than the sensitivity-based approach
                   or the deconvolution method. We provide theoretical
                   arguments to explain this result and discuss its practical
                   implications. Finally, we investigate the use of heatmaps
                   for unsupervised assessment of neural network performance.},
	Archiveprefix = {arXiv},
	Author = {Samek, Wojciech and Binder, Alexander and Montavon, Gr\'{e}goire and Bach, Sebastian and M{\"{u}}ller, Klaus-Robert},
	Eprint = {1509.06321},
	Month = {21~} # sep,
	Primaryclass = {cs.CV},
	Title = {Evaluating the visualization of what a Deep Neural Network has learned},
	Year = 2015}

@article{Bach2015-pq,
	Abstract = {Understanding and interpreting classification decisions of
                 automated image classification systems is of high value in
                 many applications, as it allows to verify the reasoning of the
                 system and provides additional information to the human
                 expert. Although machine learning methods are solving very
                 successfully a plethora of tasks, they have in most cases the
                 disadvantage of acting as a black box, not providing any
                 information about what made them arrive at a particular
                 decision. This work proposes a general solution to the problem
                 of understanding classification decisions by pixel-wise
                 decomposition of nonlinear classifiers. We introduce a
                 methodology that allows to visualize the contributions of
                 single pixels to predictions for kernel-based classifiers over
                 Bag of Words features and for multilayered neural networks.
                 These pixel contributions can be visualized as heatmaps and
                 are provided to a human expert who can intuitively not only
                 verify the validity of the classification decision, but also
                 focus further analysis on regions of potential interest. We
                 evaluate our method for classifiers trained on PASCAL VOC 2009
                 images, synthetic image data containing geometric shapes, the
                 MNIST handwritten digits data set and for the pre-trained
                 ImageNet model available as part of the Caffe open source
                 package.},
	Affiliation = {Machine Learning Group, Fraunhofer Heinrich Hertz Institute, Berlin, Germany; Machine Learning Group, Technische Universit{\"{a}}t Berlin, Berlin, Germany. Machine Learning Group, Technische Universit{\"{a}}t Berlin, Berlin, Germany; ISTD Pillar, Singapore University of Technology and Design (SUTD), Singapore. Machine Learning Group, Technische Universit{\"{a}}t Berlin, Berlin, Germany. Charit\'{e} University Hospital, Berlin, Germany. Machine Learning Group, Technische Universit{\"{a}}t Berlin, Berlin, Germany; Department of Brain and Cognitive Engineering, Korea University, Seoul, Korea. Machine Learning Group, Fraunhofer Heinrich Hertz Institute, Berlin, Germany; Machine Learning Group, Technische Universit{\"{a}}t Berlin, Berlin, Germany.},
	Author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr\'{e}goire and Klauschen, Frederick and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
	Journal = {PLoS One},
	Month = {10~} # jul,
	Number = 7,
	Pages = {e0130140},
	Title = {On {Pixel-Wise} Explanations for {Non-Linear} Classifier Decisions by {Layer-Wise} Relevance Propagation},
	Volume = 10,
	Year = 2015}

@article{Simonyan2013-hk,
	Abstract = {This paper addresses the visualisation of image
                   classification models, learnt using deep Convolutional
                   Networks (ConvNets). We consider two visualisation
                   techniques, based on computing the gradient of the class
                   score with respect to the input image. The first one
                   generates an image, which maximises the class score [Erhan
                   et al., 2009], thus visualising the notion of the class,
                   captured by a ConvNet. The second technique computes a class
                   saliency map, specific to a given image and class. We show
                   that such maps can be employed for weakly supervised object
                   segmentation using classification ConvNets. Finally, we
                   establish the connection between the gradient-based ConvNet
                   visualisation methods and deconvolutional networks [Zeiler
                   et al., 2013].},
	Archiveprefix = {arXiv},
	Author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	Eprint = {1312.6034},
	Month = {20~} # dec,
	Primaryclass = {cs.CV},
	Title = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
	Year = 2013}

@article{Chung2014-uj,
	Abstract = {In this paper we compare different types of recurrent units
                   in recurrent neural networks (RNNs). Especially, we focus on
                   more sophisticated units that implement a gating mechanism,
                   such as a long short-term memory (LSTM) unit and a recently
                   proposed gated recurrent unit (GRU). We evaluate these
                   recurrent units on the tasks of polyphonic music modeling
                   and speech signal modeling. Our experiments revealed that
                   these advanced recurrent units are indeed better than more
                   traditional recurrent units such as tanh units. Also, we
                   found GRU to be comparable to LSTM.},
	Archiveprefix = {arXiv},
	Author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
	Eprint = {1412.3555},
	Month = {11~} # dec,
	Primaryclass = {cs.NE},
	Title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
	Year = 2014}

@article{Hochreiter1997-yn,
	Abstract = {Learning to store information over extended time intervals by
                 recurrent backpropagation takes a very long time, mostly
                 because of insufficient, decaying error backflow. We briefly
                 review Hochreiter's (1991) analysis of this problem, then
                 address it by introducing a novel, efficient, gradient-based
                 method called long short-term memory (LSTM). Truncating the
                 gradient where this does not do harm, LSTM can learn to bridge
                 minimal time lags in excess of 1000 discrete-time steps by
                 enforcing constant error flow through constant error carousels
                 within special units. Multiplicative gate units learn to open
                 and close access to the constant error flow. LSTM is local in
                 space and time; its computational complexity per time step and
                 weight is O(1). Our experiments with artificial data involve
                 local, distributed, real-valued, and noisy pattern
                 representations. In comparisons with real-time recurrent
                 learning, back propagation through time, recurrent cascade
                 correlation, Elman nets, and neural sequence chunking, LSTM
                 leads to many more successful runs, and learns much faster.
                 LSTM also solves complex, artificial long-time-lag tasks that
                 have never been solved by previous recurrent network
                 algorithms.},
	Affiliation = {Fakult{\"{a}}t f{\"{u}}r Informatik, Technische Universit{\"{a}}t M{\"{u}}nchen, Germany.},
	Author = {Hochreiter, S and Schmidhuber, J},
	Journal = {Neural Comput.},
	Month = {15~} # nov,
	Number = 8,
	Pages = {1735--1780},
	Title = {Long short-term memory},
	Volume = 9,
	Year = 1997}

@article{Springenberg2014-gg,
	Abstract = {Most modern convolutional neural networks (CNNs) used for
                   object recognition are built using the same principles:
                   Alternating convolution and max-pooling layers followed by a
                   small number of fully connected layers. We re-evaluate the
                   state of the art for object recognition from small images
                   with convolutional networks, questioning the necessity of
                   different components in the pipeline. We find that
                   max-pooling can simply be replaced by a convolutional layer
                   with increased stride without loss in accuracy on several
                   image recognition benchmarks. Following this finding -- and
                   building on other recent work for finding simple network
                   structures -- we propose a new architecture that consists
                   solely of convolutional layers and yields competitive or
                   state of the art performance on several object recognition
                   datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the
                   network we introduce a new variant of the ``deconvolution
                   approach'' for visualizing features learned by CNNs, which
                   can be applied to a broader range of network structures than
                   existing approaches.},
	Archiveprefix = {arXiv},
	Author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	Eprint = {1412.6806},
	Month = {21~} # dec,
	Primaryclass = {cs.LG},
	Title = {Striving for Simplicity: The All Convolutional Net},
	Year = 2014}
